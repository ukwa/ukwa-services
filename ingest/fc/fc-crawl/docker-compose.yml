version: "3.7"

services:

  # UKWA Heritrix for NPLD
  npld-heritrix-worker:
    user: ${H3_UID}
    image: ukwa/heritrix:${HERITRIX_VERSION}
    hostname: "npld-heritrix3-worker-{{.Task.Slot}}"
    ports:
      - 8443:8443
    env_file: heritrix-shared-settings.env
    environment:
      - "KAFKA_TOCRAWL_TOPIC=fc.tocrawl.npld"
      - "KAFKA_INSCOPE_TOPIC=fc.inscope.npld"
      - "KAFKA_CANDIDATES_TOPIC=fc.tocrawl.npld"
      - "CRAWL_NAME=frequent-npld"
      - "WARC_PREFIX=BL-NPLD"
      - "WEBRENDER_WARC_PREFIX=BL-NPLD-WEBRENDER"
      - "CDXSERVER_ENDPOINT=${CDXSERVER_ENDPOINT}"
    volumes:
      - "${HERITRIX_OUTPUT_PATH}:/heritrix/output"
      - "${NPLD_STATE_PATH}:/heritrix/state"
      - "${SURTS_NPLD_PATH}:/shared" # Shared configuration - surts file held here.
    deploy:
      replicas: 1
    ulimits:
      nproc: 65535
      nofile:
        soft: 204800
        hard: 204800
    stop_grace_period: 5m # Give the H3 instances some time to shut down neatly following SIGTERM
    depends_on:
      - clamd
      - webrender
      - crawler-cdx
    networks:
      - default
      - kafka

  # UKWA Heritrix for BYPM
  bypm-heritrix-worker:
    user: ${H3_UID}
    image: ukwa/heritrix:${HERITRIX_VERSION}
    hostname: "bypm-heritrix3-worker-{{.Task.Slot}}"
    ports:
      - 9443:8443
    env_file: heritrix-shared-settings.env
    environment:
      - "KAFKA_TOCRAWL_TOPIC=fc.tocrawl.bypm"
      - "KAFKA_INSCOPE_TOPIC=fc.inscope.bypm"
      - "KAFKA_CANDIDATES_TOPIC=fc.tocrawl.bypm"
      - "CRAWL_NAME=frequent-bypm"
      - "WARC_PREFIX=BL-BYPM"
      - "WEBRENDER_WARC_PREFIX=BL-BYPM-WEBRENDER"
      - "CDXSERVER_ENDPOINT=${CDXSERVER_ENDPOINT}"
    volumes:
      - "${HERITRIX_OUTPUT_PATH}:/heritrix/output"
      - "${BYPM_STATE_PATH}:/heritrix/state"
      - "${SURTS_BYPM_PATH}:/shared" # Shared configuration - surts file held here.
    deploy:
      replicas: 1
    ulimits:
      nproc: 65535
      nofile:
        soft: 204800
        hard: 204800
    stop_grace_period: 5m # Give the H3 instances some time to shut down neatly following SIGTERM
    depends_on:
      - clamd
      - webrender
      - crawler-cdx
    networks:
      - default
      - kafka


  # Clamd virus scanning Service
  clamd:
    image: ukwa/clamd
    deploy:
      replicas: 8

  # CDX server for deduplication and last-crawled-at:
  crawler-cdx:
    image: nlagovau/outbackcdx:0.11.0
    command: "java -Xmx2g -jar /outbackcdx.jar -d /crawldb-fc -b 0.0.0.0 -p 8081 -t 1000 -u"
    ports:
      - "8081:8081"
    volumes:
      - ${CDX_STORAGE_PATH}:/crawldb-fc
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "5"


  # ----------------------------------------------
  # WARC Proxy
  # ----------------------------------------------

  # The WARChiving proxy itself
  warcprox:
    image: ukwa/docker-warcprox:2.4.17.2
    command: "warcprox -b 0.0.0.0 -d /output/warcs --base32 --gzip --rollover-idle-time 600 --dedup-db-file /host/warcprox_dedup_db.sqlite --stats-db-file /dev/null --quiet --plugin warcprox-plugins.listeners.UpdateOutbackCDX --plugin warcprox-plugins.listeners.KafkaCaptureFeed --max-threads 1000 --queue-size 1000"
    environment:
      - "KAFKA_BOOTSTRAP_SERVERS=kafka:9092"
      - "KAFKA_CRAWLED_TOPIC=fc.crawled"
      - "KAFKA_ACKS=1"
      - "CDXSERVER_ENDPOINT=${CDXSERVER_ENDPOINT}"
    volumes:
      - "${HERITRIX_WREN_PATH}:/output/warcs"
      - "${WARCPROX_PATH}:/host" # FAST disk for dedup db
    ulimits:
      nproc: 2000  # See https://github.com/internetarchive/warcprox/blob/2.x/warcprox/warcproxy.py#L413
      nofile:
        soft: 204800
        hard: 204800
    deploy:
      replicas: 1
    depends_on:
      - crawler-cdx
    networks:
      - default
      - kafka


  # ----------------------------------------------
  # Web Render Farm
  # ----------------------------------------------

  # Puppeteer web page rendering service
  webrender:
    image: ukwa/webrender-puppeteer:2.3.3
    # Can't use this with dnsrr
    #ports:
    #  - 8010:8010
    environment:
      - "HTTP_PROXY=http://warcprox:8000"
      - "WARCPROX_PROXY=true"
      - "LC_ALL=en_US.utf8"
      - "PORT=8010"
      - "NODE_ENV=production"
      - "PUPPETEER_CLUSTER_SIZE=4"
      # The renderer supports substituting the @VERSION@
      - "USER_AGENT_ADDITIONAL=bl.uk_ldfc_renderbot/@VERSION@ (+https://www.bl.uk/legal-deposit/web-archiving)"
    volumes:
      - "${STORAGE_PATH}/heritrix/wren:/output/warcs"
    ulimits:
      nproc: 10240 # Those browsers need a lot of processes and threads
    deploy:
      # Use this to avoid the Ingress network, as it appears to forcibly reset long-running connections.
      endpoint_mode: dnsrr
      # Note that using dnsrr means we can't use the ingress network for load balancing, and any load balancing has to be done explicitly.
      replicas: 1
      # Limit resources to avoid swamping the server:
      resources:
        limits:
          cpus: '4.0'
          memory: 4G
    depends_on:
      - warcprox
    networks:
      - default
      - kafka



  # ----------------------------------------------
  # Monitoring
  # ----------------------------------------------

  prometheus:
    image: prom/prometheus
    ports:
      - 9191:9090
    volumes:
      - ./prometheus:/etc/prometheus
      - "${STORAGE_PATH}/prometheus-data:/prometheus"
    user: root
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=1000d'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
      - '--web.external-url=http://${CRAWL_HOST_LAN_IP}:9191/'
      - '--web.enable-admin-api'
      - '--web.enable-lifecycle'
    networks:
      - default
      - kafka

networks:
  # Allow attachment of external monitoring:
  default:
    driver: overlay
    attachable: true
  # Link to Kafka service:
  kafka:
    external: true
    name: fc_kafka_default

